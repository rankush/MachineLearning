{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall Detection\n",
    "Machine learning algorithm to detect fall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global variables\n",
    "- Import statements\n",
    "- Global Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from csv import reader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "FILEPATH_MOTION_DATASET = \"../logs/motion/\"\n",
    "FILEPATH_STATIONARY_DATASET = \"../logs/stationary/\"\n",
    "\n",
    "# Defining the output\n",
    "STATIONARY = 0\n",
    "MOTION = 1\n",
    "\n",
    "# Max value is used for normalization\n",
    "MAX_ACCEL = 20\n",
    "\n",
    "# Test Size\n",
    "TEST_SIZE = 0.30 # 30 percent\n",
    "\n",
    "# Hyper Parameters\n",
    "input_dim = 150 #x*50, y*50, z*50\n",
    "output_dim = nb_classes = 2 # 2 classes\n",
    "batch_size = 1 \n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "- Load the dataset\n",
    "\n",
    "1. Extract only the linear accel (last 3 entries)\n",
    "2. Limit the size to 50 (#25 to #75)\n",
    "3. Normalize the data (-20 to 20) will get changed to (-1 to 1). All values are clipped at 20\n",
    "4. Flatten the data - looks like [x1,y1,z1,x2,y2,z2...,x150,y150,z150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 150)\n",
      "2250\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    file = open(filename, \"r\")\n",
    "    lines = reader(file)\n",
    "    dataset = list(lines)\n",
    "    return np.float_(dataset)\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # extract last 3 entries and limit size to 50\n",
    "    data = data[24:74,3:6]\n",
    "    data = np.clip(data, -1 * MAX_ACCEL, MAX_ACCEL)\n",
    "    data = data/MAX_ACCEL\n",
    "    return data.flatten()\n",
    "\n",
    "def convert_file_to_data_array(filepath):\n",
    "    return preprocess_data(load_csv(filepath))\n",
    "\n",
    "# print(\"First five samples read from file\")\n",
    "# print(load_csv(\"../logs/motion/1.txt\")[:5])\n",
    "\n",
    "# print(\"First five samples preprocessing\")\n",
    "# print(convert_file_to_data_array(\"../logs/motion/1.txt\")[:5])\n",
    "\n",
    "def load_dataset():\n",
    "    Y = []\n",
    "\n",
    "    stationary_files = os.listdir(FILEPATH_STATIONARY_DATASET)\n",
    "    motion_files = os.listdir(FILEPATH_MOTION_DATASET)\n",
    "    X = np.zeros(shape=(len(stationary_files) + len(motion_files),input_dim))\n",
    "    x_index = 0\n",
    "\n",
    "    for file in stationary_files:\n",
    "        array = convert_file_to_data_array(FILEPATH_STATIONARY_DATASET+file)\n",
    "        if array.shape[0] is input_dim:\n",
    "            X[x_index] = array\n",
    "            x_index = x_index +1 \n",
    "            Y.append(STATIONARY)\n",
    "\n",
    "    for file in os.listdir(FILEPATH_MOTION_DATASET):\n",
    "        array = convert_file_to_data_array(FILEPATH_MOTION_DATASET+file)\n",
    "        if array.shape[0] is input_dim:\n",
    "            X[x_index] = array\n",
    "            x_index = x_index +1 \n",
    "            Y.append(MOTION)\n",
    "    \n",
    "    #print (x_index)\n",
    "    return X[:len(Y)],Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset\n",
    "### Convert class vectors to binary class matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 150)\n",
      "2250\n",
      "(10, 150)\n",
      "(5, 150)\n"
     ]
    }
   ],
   "source": [
    "(X, y) = load_dataset()\n",
    "#print(X)\n",
    "print (np.array(X).shape)\n",
    "# print (np.array(X).size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE)\n",
    "\n",
    "# print(X_train[:2])\n",
    "# print(y_train[:2])\n",
    "# print(y_test)\n",
    "\n",
    "# print (np.array(X_train).shape)\n",
    "# print (np.array(X_test).shape)\n",
    "\n",
    "#X_train = np.array(X_train).reshape(11, input_dim) \n",
    "#X_test = np.array(X_test).reshape(5, input_dim) \n",
    "# print (np.array(X_train).)\n",
    "# print (X_test.shape)\n",
    "# print (y_train.shape)\n",
    "# print (y_test.shape)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes) \n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 302       \n",
      "=================================================================\n",
      "Total params: 302\n",
      "Trainable params: 302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 5 samples\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.7749 - acc: 0.4000 - val_loss: 0.6504 - val_acc: 0.8000\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.7147 - acc: 0.3000 - val_loss: 0.6386 - val_acc: 0.4000\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 836us/step - loss: 0.6752 - acc: 0.4000 - val_loss: 0.6263 - val_acc: 0.4000\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 963us/step - loss: 0.6444 - acc: 0.4000 - val_loss: 0.6177 - val_acc: 0.4000\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.6171 - acc: 0.5000 - val_loss: 0.5967 - val_acc: 0.4000\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.5977 - acc: 0.5000 - val_loss: 0.5805 - val_acc: 0.4000\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.5782 - acc: 0.5000 - val_loss: 0.5673 - val_acc: 0.4000\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.5580 - acc: 0.7000 - val_loss: 0.5599 - val_acc: 0.4000\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 773us/step - loss: 0.5437 - acc: 0.5000 - val_loss: 0.5400 - val_acc: 0.8000\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 896us/step - loss: 0.5278 - acc: 0.7000 - val_loss: 0.5240 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 946us/step - loss: 0.5133 - acc: 0.7000 - val_loss: 0.5044 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 830us/step - loss: 0.5001 - acc: 0.9000 - val_loss: 0.4896 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 952us/step - loss: 0.4861 - acc: 0.9000 - val_loss: 0.4750 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 826us/step - loss: 0.4736 - acc: 0.9000 - val_loss: 0.4580 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 908us/step - loss: 0.4616 - acc: 0.9000 - val_loss: 0.4492 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 811us/step - loss: 0.4490 - acc: 1.0000 - val_loss: 0.4308 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 874us/step - loss: 0.4400 - acc: 0.9000 - val_loss: 0.4199 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 889us/step - loss: 0.4291 - acc: 0.9000 - val_loss: 0.4117 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.4191 - acc: 0.9000 - val_loss: 0.4028 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 785us/step - loss: 0.4094 - acc: 0.9000 - val_loss: 0.3949 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.3949466645717621\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
